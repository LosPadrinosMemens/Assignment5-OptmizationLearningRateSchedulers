{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from Optimization import *\n",
    "from opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the a model and datasets (a complex classification problem that you can choose on your own) and test different regularization techniques:\n",
    "\n",
    "Here we will be using the same dataset as the assignment 2 in which the problem is to classify a set of images as cats or not. \n",
    "Although it is not a requirement from this assignment, we will first try to solve this problem without regularization whatsoever.\n",
    "\n",
    "To follow the same idea from the assignment 2, for the training of the model we will be using the given dataset by the teacher as a file named train_catvnoncat.h5. While the first 100 images from the sets of cats and dogs labeled as \"train\" retrieved from https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?select=train will be used for our testing. And the next 100 images from those that same sets will be used for our tuning set. We will first process them to reduce their pixel resolution to the same as the ones we used in the training (64 pixels x 64 pixels) and then represent them as arrays.\n",
    "##### 0.  Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions and packages for processing and loading the datasets:\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset_flatten = dataset.reshape(dataset.shape[0],-1).T\n",
    "    return dataset_flatten/255 \n",
    "\n",
    "train_set_x = preprocess_dataset(train_set_x_orig) \n",
    "test_set_x = preprocess_dataset(test_set_x_orig)\n",
    "\n",
    "## Extract our own images from cats and dogs\n",
    "import os\n",
    "\n",
    "def process_images(directory, target_size=(64, 64), image_range=range(0,100)):\n",
    "    image_list = []\n",
    "\n",
    "    filenames = sorted(os.listdir(directory))[min(image_range):max(image_range)+1]\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            img = Image.open(file_path)\n",
    "            img_resized = img.resize(target_size)\n",
    "            img_array = np.array(img_resized)\n",
    "            \n",
    "            image_list.append(img_array)\n",
    "    \n",
    "    return np.array(image_list)\n",
    "\n",
    "test_cat_images = process_images(\"datasets/train/cats\")\n",
    "test_dog_images = process_images(\"datasets/train/dogs\")\n",
    "tuning_cat_images = process_images(\"datasets/train/cats\",image_range=(101,200))\n",
    "tuning_dog_images = process_images(\"datasets/train/dogs\",image_range=(101,200))\n",
    "\n",
    "def join_cats_and_dogs(cat_images,dog_images):\n",
    "    cat_set_X = preprocess_dataset(cat_images)\n",
    "    dog_set_X = preprocess_dataset(dog_images)\n",
    "    m_cat_set = cat_set_X.shape[1]\n",
    "    cat_set_Y = np.ones((1, m_cat_set))\n",
    "    m_dog_set = dog_set_X.shape[1]\n",
    "    dog_set_Y = np.zeros((1, m_dog_set))\n",
    "\n",
    "    set_X = np.concatenate((cat_set_X, dog_set_X), axis=1)\n",
    "    set_Y = np.concatenate((cat_set_Y, dog_set_Y), axis=1)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    shuffle_indices = np.random.permutation(set_X.shape[1])\n",
    "    final_set_X = set_X[:, shuffle_indices]\n",
    "    final_set_Y = set_Y[:, shuffle_indices]\n",
    "\n",
    "    return final_set_X,final_set_Y\n",
    "\n",
    "final_test_X, final_test_Y = join_cats_and_dogs(test_cat_images,test_dog_images)\n",
    "final_tuning_X, final_tuning_Y = join_cats_and_dogs(tuning_cat_images,tuning_dog_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Use the a model (shallow fully connected network) and datasets (a complex classification problem that you can choose on your own) and test the following optimizers in the best configuration that you have obtained from Assignement 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same architecture for this network as the one from the last assignments given by the next formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers_dims = [X.shape[0], 20, 3, 1]\n",
    "\n",
    "layers_dims = [train_set_x.shape[0], 20, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13042\\Desktop\\Maestria\\Semestre 3\\Neural Systems\\Assignment5-OptmizationLearningRateSchedulers\\Optimization.py:62\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(X, Y, layers_dims, optimizer, learning_rate, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[0;32m     59\u001b[0m m \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]                   \u001b[38;5;66;03m# number of training examples\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Initialize parameters\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_parameters\u001b[49m(layers_dims)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Initialize the optimizer\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "model(train_set_x, train_set_y, layers_dims, \"gd\", learning_rate = 0.005, num_epochs = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Momentum\n",
    "\n",
    "### 3. Adam\n",
    " \n",
    "### Show the effects of the optimizer on the final accuracy and validation loss (plot the training and validation loss)\n",
    " \n",
    "### b) Test the three models using the two basic learning rate schedulers\n",
    " \n",
    "### 1. Expontenial LR Decay (provided in the notebook)\n",
    " \n",
    "### 2. Step LR Decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
